{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/Wonderland.txt'\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  163815\n",
      "Total Vocab:  60\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print( \"Total Characters: \", n_chars)\n",
    "print( \"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  163715\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"data/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "163712/163715 [============================>.] - ETA: 0s - loss: 2.8178Epoch 00001: loss improved from 2.99171 to 2.81782, saving model to weights-improvement-01-2.8178.hdf5\n",
      "163715/163715 [==============================] - 1048s 6ms/step - loss: 2.8178\n",
      "Epoch 2/20\n",
      "163712/163715 [============================>.] - ETA: 0s - loss: 2.7197Epoch 00002: loss improved from 2.81782 to 2.71971, saving model to weights-improvement-02-2.7197.hdf5\n",
      "163715/163715 [==============================] - 1069s 7ms/step - loss: 2.7197\n",
      "Epoch 3/20\n",
      "163712/163715 [============================>.] - ETA: 0s - loss: 2.6482Epoch 00003: loss improved from 2.71971 to 2.64820, saving model to weights-improvement-03-2.6482.hdf5\n",
      "163715/163715 [==============================] - 1066s 7ms/step - loss: 2.6482\n",
      "Epoch 4/20\n",
      "163712/163715 [============================>.] - ETA: 0s - loss: 2.5821Epoch 00004: loss improved from 2.64820 to 2.58216, saving model to weights-improvement-04-2.5822.hdf5\n",
      "163715/163715 [==============================] - 1075s 7ms/step - loss: 2.5822\n",
      "Epoch 5/20\n",
      "163712/163715 [============================>.] - ETA: 0s - loss: 2.5211Epoch 00005: loss improved from 2.58216 to 2.52107, saving model to weights-improvement-05-2.5211.hdf5\n",
      "163715/163715 [==============================] - 1062s 6ms/step - loss: 2.5211\n",
      "Epoch 6/20\n",
      "163712/163715 [============================>.] - ETA: 0s - loss: 2.4672Epoch 00006: loss improved from 2.52107 to 2.46719, saving model to weights-improvement-06-2.4672.hdf5\n",
      "163715/163715 [==============================] - 1060s 6ms/step - loss: 2.4672\n",
      "Epoch 7/20\n",
      "163712/163715 [============================>.] - ETA: 0s - loss: 2.4207Epoch 00007: loss improved from 2.46719 to 2.42069, saving model to weights-improvement-07-2.4207.hdf5\n",
      "163715/163715 [==============================] - 1079s 7ms/step - loss: 2.4207\n",
      "Epoch 8/20\n",
      "163712/163715 [============================>.] - ETA: 0s - loss: 2.3719Epoch 00008: loss improved from 2.42069 to 2.37192, saving model to weights-improvement-08-2.3719.hdf5\n",
      "163715/163715 [==============================] - 1067s 7ms/step - loss: 2.3719\n",
      "Epoch 9/20\n",
      "163712/163715 [============================>.] - ETA: 0s - loss: 2.3297Epoch 00009: loss improved from 2.37192 to 2.32973, saving model to weights-improvement-09-2.3297.hdf5\n",
      "163715/163715 [==============================] - 1072s 7ms/step - loss: 2.3297\n",
      "Epoch 10/20\n",
      "163712/163715 [============================>.] - ETA: 0s - loss: 2.2901Epoch 00010: loss improved from 2.32973 to 2.29014, saving model to weights-improvement-10-2.2901.hdf5\n",
      "163715/163715 [==============================] - 1052s 6ms/step - loss: 2.2901\n",
      "Epoch 11/20\n",
      "163712/163715 [============================>.] - ETA: 0s - loss: 2.2515Epoch 00011: loss improved from 2.29014 to 2.25150, saving model to weights-improvement-11-2.2515.hdf5\n",
      "163715/163715 [==============================] - 1051s 6ms/step - loss: 2.2515\n",
      "Epoch 12/20\n",
      "163712/163715 [============================>.] - ETA: 0s - loss: 2.2183Epoch 00012: loss improved from 2.25150 to 2.21834, saving model to weights-improvement-12-2.2183.hdf5\n",
      "163715/163715 [==============================] - 1050s 6ms/step - loss: 2.2183\n",
      "Epoch 13/20\n",
      "163712/163715 [============================>.] - ETA: 0s - loss: 2.1864Epoch 00013: loss improved from 2.21834 to 2.18638, saving model to weights-improvement-13-2.1864.hdf5\n",
      "163715/163715 [==============================] - 1050s 6ms/step - loss: 2.1864\n",
      "Epoch 14/20\n",
      "163712/163715 [============================>.] - ETA: 0s - loss: 2.1557Epoch 00014: loss improved from 2.18638 to 2.15566, saving model to weights-improvement-14-2.1557.hdf5\n",
      "163715/163715 [==============================] - 1051s 6ms/step - loss: 2.1557\n",
      "Epoch 15/20\n",
      "163712/163715 [============================>.] - ETA: 0s - loss: 2.1249Epoch 00015: loss improved from 2.15566 to 2.12486, saving model to weights-improvement-15-2.1249.hdf5\n",
      "163715/163715 [==============================] - 1101s 7ms/step - loss: 2.1249\n",
      "Epoch 16/20\n",
      "163712/163715 [============================>.] - ETA: 0s - loss: 2.0985Epoch 00016: loss improved from 2.12486 to 2.09852, saving model to weights-improvement-16-2.0985.hdf5\n",
      "163715/163715 [==============================] - 1100s 7ms/step - loss: 2.0985\n",
      "Epoch 17/20\n",
      "163712/163715 [============================>.] - ETA: 0s - loss: 2.0709Epoch 00017: loss improved from 2.09852 to 2.07095, saving model to weights-improvement-17-2.0709.hdf5\n",
      "163715/163715 [==============================] - 1081s 7ms/step - loss: 2.0709\n",
      "Epoch 18/20\n",
      "163712/163715 [============================>.] - ETA: 0s - loss: 2.0492Epoch 00018: loss improved from 2.07095 to 2.04918, saving model to weights-improvement-18-2.0492.hdf5\n",
      "163715/163715 [==============================] - 1060s 6ms/step - loss: 2.0492\n",
      "Epoch 19/20\n",
      "163712/163715 [============================>.] - ETA: 0s - loss: 2.0237Epoch 00019: loss improved from 2.04918 to 2.02368, saving model to weights-improvement-19-2.0237.hdf5\n",
      "163715/163715 [==============================] - 1048s 6ms/step - loss: 2.0237\n",
      "Epoch 20/20\n",
      "163712/163715 [============================>.] - ETA: 0s - loss: 2.0062Epoch 00020: loss improved from 2.02368 to 2.00619, saving model to weights-improvement-20-2.0062.hdf5\n",
      "163715/163715 [==============================] - 1048s 6ms/step - loss: 2.0062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe974430240>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/weights-improvement-19-1.9435.hdf5\"\n",
    "model.save_weights(filename)\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\"  from her as hard as it could go, and making quite a commotion in\n",
      "the pool as it went.\n",
      "\n",
      "so she calle \"\n",
      "d to ter whate whe world oadt to the woile oa then and toen a getd wan a lootne oo the tait, and the war not in the bane and the sabbit sat sh the tabte oi the gad nerer here the was so toe thee  the was not in the bane and the sabbit sat sh the tabte oi the garter was no the tine the was soenk again, and the whrt here toiee the whrt wall to the woide the was so tone the was so toye \n",
      "the had nov oo the taate of the war so toenk ano hor hoa to the tooe of the garter, and was solniig an inr an she wooke. and tae toei it har hnat the white rabbit was soe oant of the whrt of the garter, and whs wored toen it tas an anl oo the toie. ‘ho wou den to you do wou hane to tea that iave a can! io wou den’ wouhd the want on an inrssns,’\n",
      "\n",
      "‘h woode than to ae a cetd wail to,’ said alice, ‘aod the mors oi the garter wouh a cian winh i sai soene the wante  and the world bale wait in she bane and the sare whth the same th thi white rase in and toen a gett wanlee of the woide oo the wan hoon the tas so t\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print( \"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print( \"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#influenced by https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
