{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, TimeDistributed, Dense, RepeatVector, recurrent, Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(source, dist, max_len, vocab_size):\n",
    "\n",
    "    # Reading raw text from source and destination files\n",
    "    f = open(source, 'r')\n",
    "    X_data = f.read()\n",
    "    f.close()\n",
    "    f = open(dist, 'r')\n",
    "    y_data = f.read()\n",
    "    f.close()\n",
    "\n",
    "    # Splitting raw text into array of sequences\n",
    "    X = [text_to_word_sequence(x)[::-1] for x, y in zip(X_data.split('\\n'), y_data.split('\\n')) if len(x) > 0 and len(y) > 0 and len(x) <= max_len and len(y) <= max_len]\n",
    "    y = [text_to_word_sequence(y) for x, y in zip(X_data.split('\\n'), y_data.split('\\n')) if len(x) > 0 and len(y) > 0 and len(x) <= max_len and len(y) <= max_len]\n",
    "\n",
    "    # Creating the vocabulary set with the most common words\n",
    "    dist = FreqDist(np.hstack(X))\n",
    "    X_vocab = dist.most_common(vocab_size-1)\n",
    "    dist = FreqDist(np.hstack(y))\n",
    "    y_vocab = dist.most_common(vocab_size-1)\n",
    "\n",
    "    # Creating an array of words from the vocabulary set, we will use this array as index-to-word dictionary\n",
    "    X_ix_to_word = [word[0] for word in X_vocab]\n",
    "    # Adding the word \"ZERO\" to the beginning of the array\n",
    "    X_ix_to_word.insert(0, 'ZERO')\n",
    "    # Adding the word 'UNK' to the end of the array (stands for UNKNOWN words)\n",
    "    X_ix_to_word.append('UNK')\n",
    "\n",
    "    # Creating the word-to-index dictionary from the array created above\n",
    "    X_word_to_ix = {word:ix for ix, word in enumerate(X_ix_to_word)}\n",
    "\n",
    "    # Converting each word to its index value\n",
    "    for i, sentence in enumerate(X):\n",
    "        for j, word in enumerate(sentence):\n",
    "            if word in X_word_to_ix:\n",
    "                X[i][j] = X_word_to_ix[word]\n",
    "            else:\n",
    "                X[i][j] = X_word_to_ix['UNK']\n",
    "\n",
    "    y_ix_to_word = [word[0] for word in y_vocab]\n",
    "    y_ix_to_word.insert(0, 'ZERO')\n",
    "    y_ix_to_word.append('UNK')\n",
    "    y_word_to_ix = {word:ix for ix, word in enumerate(y_ix_to_word)}\n",
    "    for i, sentence in enumerate(y):\n",
    "        for j, word in enumerate(sentence):\n",
    "            if word in y_word_to_ix:\n",
    "                y[i][j] = y_word_to_ix[word]\n",
    "            else:\n",
    "                y[i][j] = y_word_to_ix['UNK']\n",
    "    return (X, len(X_vocab)+2, X_word_to_ix, X_ix_to_word, y, len(y_vocab)+2, y_word_to_ix, y_ix_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(source, X_word_to_ix, max_len):\n",
    "    f = open(source, 'r')\n",
    "    X_data = f.read()\n",
    "    f.close()\n",
    "\n",
    "    X = [text_to_word_sequence(x)[::-1] for x in X_data.split('\\n') if len(x) > 0 and len(x) <= max_len]\n",
    "    for i, sentence in enumerate(X):\n",
    "        for j, word in enumerate(sentence):\n",
    "            if word in X_word_to_ix:\n",
    "                X[i][j] = X_word_to_ix[word]\n",
    "            else:\n",
    "                X[i][j] = X_word_to_ix['UNK']\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(X_vocab_len, X_max_len, y_vocab_len, y_max_len, hidden_size, num_layers):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Creating encoder network\n",
    "    model.add(Embedding(X_vocab_len, 1000, input_length=X_max_len, mask_zero=True))\n",
    "    model.add(LSTM(hidden_size))\n",
    "    model.add(RepeatVector(y_max_len))\n",
    "\n",
    "    # Creating decoder network\n",
    "    for _ in range(num_layers):\n",
    "        model.add(LSTM(hidden_size, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(y_vocab_len)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "            optimizer='rmsprop',\n",
    "            metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(word_sentences, max_len, word_to_ix):\n",
    "    # Vectorizing each element in each sequence\n",
    "    sequences = np.zeros((len(word_sentences), max_len, len(word_to_ix)))\n",
    "    for i, sentence in enumerate(word_sentences):\n",
    "        for j, word in enumerate(sentence):\n",
    "            sequences[i, j, word] = 1.\n",
    "    return sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_checkpoint_file(folder):\n",
    "    checkpoint_file = [f for f in os.listdir(folder) if 'checkpoint' in f]\n",
    "    if len(checkpoint_file) == 0:\n",
    "        return []\n",
    "    modified_time = [os.path.getmtime(f) for f in checkpoint_file]\n",
    "    return checkpoint_file[np.argmax(modified_time)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 200\n",
    "VOCAB_SIZE = 20000\n",
    "BATCH_SIZE = 1000\n",
    "LAYER_NUM = 3\n",
    "HIDDEN_DIM = 1000\n",
    "NB_EPOCH = 20\n",
    "MODE = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'europarl-v8.fi-en.en'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e8f6ee378a89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[INFO] Loading data...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_vocab_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_word_to_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_ix_to_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_vocab_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_word_to_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ix_to_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'europarl-v8.fi-en.en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'europarl-v8.fi-en.fi'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Finding the length of the longest sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_max_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-9d57fdbde253>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(source, dist, max_len, vocab_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Reading raw text from source and destination files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mX_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'europarl-v8.fi-en.en'"
     ]
    }
   ],
   "source": [
    "print('[INFO] Loading data...')\n",
    "X, X_vocab_len, X_word_to_ix, X_ix_to_word, y, y_vocab_len, y_word_to_ix, y_ix_to_word = load_data('europarl-v8.fi-en.en', 'europarl-v8.fi-en.fi', MAX_LEN, VOCAB_SIZE)\n",
    "\n",
    "    # Finding the length of the longest sequence\n",
    "X_max_len = max([len(sentence) for sentence in X])\n",
    "y_max_len = max([len(sentence) for sentence in y])\n",
    "\n",
    "    # Padding zeros to make all sequences have a same length with the longest one\n",
    "print('[INFO] Zero padding...')\n",
    "X = pad_sequences(X, maxlen=X_max_len, dtype='int32')\n",
    "y = pad_sequences(y, maxlen=y_max_len, dtype='int32')\n",
    "\n",
    "    # Creating the network model\n",
    "print('[INFO] Compiling model...')\n",
    "model = create_model(X_vocab_len, X_max_len, y_vocab_len, y_max_len, HIDDEN_DIM, LAYER_NUM)\n",
    "\n",
    "    # Finding trained weights of previous epoch if any\n",
    "saved_weights = find_checkpoint_file('.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
